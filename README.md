# RLProject_StateEmbeddings â˜„ï¸

A research framework for learning and evaluating state embeddings in reinforcement learning (RL) environments. This project provides tools for pretraining, training, and evaluating RL agents with learned state representations, including transformer-based embeddings and reconstruction objectives.

## Features ğŸª

- **Transformer-based State Embeddings:** Learn compact representations of environment states using transformer encoders.
- **Reconstruction Objectives:** Jointly train RL agents with state reconstruction losses.
- **Flexible Environment Wrappers:** Context and embedding wrappers for stacking observations and using learned embeddings.
- **Pretraining & Evaluation:** Pretrain embeddings with various objectives and evaluate with linear probes.
- **Support for Multiple Environments:** Includes experiments for CartPole, MinAtar Breakout, and MiniGrid Unlock.

## Repository Structure ğŸŒœ

```
RLProject_StateEmbeddings/ 
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                                 # Entry point for running experiments
â”‚   â”œâ”€â”€ breakout_experiment.py                  # Breakout (MinAtar) experiment routines
â”‚   â”œâ”€â”€ cartpole_experiment.py                  # CartPole experiment routines
â”‚   â”œâ”€â”€ minigrid_memory_experiment.py           # MiniGrid Memory experiment routines
â”‚   â”œâ”€â”€ minigrid_unlock_experiment.py           # MiniGrid Unlock experiment routines
â”‚   â”œâ”€â”€ seaquest_experiment.py                  # MinAtar Seaquest experiment routines
â”‚   â”œâ”€â”€ seaquestmarkov_unlock_experiment.py     # MinAtar Seaquest_alt experiment routines
â”‚   â”œâ”€â”€ state_embedding/
â”‚       â”œâ”€â”€ env.py                              # Context and embedding environment wrappers
â”‚       â”œâ”€â”€ embedding.py                        # StateEmbedding and StateDecoder modules
â”‚       â”œâ”€â”€ embedding_eval.py                   # Linear probe for embedding evaluation
â”‚       â”œâ”€â”€ train.py                            # Pretraining routines for embeddings
â”‚       â”œâ”€â”€ callbacks.py                        # Custom RL training callbacks
â”‚       â””â”€â”€ dqn/
â”‚           â”œâ”€â”€ dqn.py                          # DQN with reconstruction loss
â”‚           â””â”€â”€ qnetwork.py                     # Q-network with embedding and reconstruction
â”‚   â””â”€â”€ envs/
â”‚       â””â”€â”€ seaquest_markov.py                  # Implementation of an alternative Seaquest env
â”‚
â””â”€â”€ README.md                                   # This file
```

## Installation ğŸŒŒ

This project uses [uv](https://github.com/astral-sh/uv) for Python package management and virtual environments.

1. **Clone this repo:**
    ```bash
    git clone https://github.com/thibautklenke/RLProject_StateEmbeddings
    cd RLProject_StateEmbeddings
    ```

2. **Create and activate a virtual environment:**
    ```bash
    uv venv
    source .venv/bin/activate
    ```

3. **Install dependencies:**
    ```bash
    uv sync
    ```

## Usage ğŸ¥¢

### Running Experiments

The main entry point is `src/main.py`, which manages pretraining and training for all supported environments.

To run all experiments (pretraining and training for each environment and seed):

```bash
uv run main
```

- By default, this will run pretraining once per environment, then train agents with different seeds.
- You can comment/uncomment lines in `main.py` to select which environments to run.

### Customizing Experiments 

- **Add new environments:** Create a new `*_experiment.py` file following the structure of `cartpole_experiment.py`.
- **Change embedding architecture:** Modify `state_embedding/embedding.py` or pass different `embedding_kwargs` in experiment files.
- **Adjust training parameters:** Edit variables like `n_pretrain`, `n_train`, or `net_arch` in the experiment scripts.


---